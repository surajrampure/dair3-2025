{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbeb756",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "from lec_utils import *\n",
    "plotly.io.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ef119",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### DAIR-3 Workshop, Day 2 â€¢ Building Robust ML Models\n",
    "\n",
    "# Part 2: Dimensionality Reduction\n",
    "\n",
    "**Instructor**: Suraj Rampure (rampure@umich.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ade72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Seeds and stratification in train-test splits.\n",
    "- PCA.\n",
    "- Other dimensionality reduction techniques.\n",
    "    - MDS.\n",
    "    - t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e877b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Seeds in train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03b9f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Run the cell below to import the breast cancer dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd583194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "full = load_breast_cancer()\n",
    "df = pd.DataFrame(full['data'], columns=full['feature_names'])\n",
    "df['target'] = 1 - full['target']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5082b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #1: Missing Random Seeds</h3>\n",
    "    \n",
    "When building a predictive model, **always set a random seed** when splitting the data into training and testing sets. The splitting is done randomly; a seed ensures the same results every time, so others can reproduce your work.\n",
    "    \n",
    "<br>\n",
    "    \n",
    "Pick a random seed arbitrarily and stick with it; **don't** intentionally select a seed that yields \"good\" results, as this will lead to an overly confident assessment of model performance.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcb92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), \n",
    "                                                    df['target'],\n",
    "                                                    random_state=23)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79679b11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- All model development should be done on the training data only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b034642",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93631f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Peek at the distribution of malignant (1) and benign (0) tumors in both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows that 38.49% of patients in the training set have a malignant tumor. Why?\n",
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbca8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Malignant tumors are not particularly rare in this dataset.<br>If they were, and we wanted to ensure the same frequency of malignant tumors in both our training and test sets, use the `stratify` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd9845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], random_state=23, stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffde324",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012d279",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7755d54a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The breast cancer dataset has 30 **features** (input variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b5090",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Many of these features may be redundant or noisy.<br>It's also impossible to visualize the entire dataset as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41976f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ee4ed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By converting this $(426 \\times 30)$ dataset to a $(426 \\times p)$ dataset, for some relatively small $p$, we can attempt to:\n",
    "    - Preserve the majority of the signal in the dataset.\n",
    "    - Add the ability to visualize the data.\n",
    "    \n",
    "<center><img src=\"images/dim-red.svg\" width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe796cd5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Dimensionality reduction is a form of **unsupervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071795f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d229fcfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d386591",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Principal component analysis (PCA) is one dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38810764",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It creates $p$ **new features**, each of which is a **linear combination** of all 30 existing features.\n",
    "\n",
    "\n",
    "    $$\\text{new feature 1} = 0.05 \\cdot \\text{mean radius} + 0.93 \\cdot \\text{mean texture} + ... - \\: 0.35 \\cdot \\text{worst fractal dimension}$$\n",
    "\n",
    "    $$\\text{new feature 2} = - 0.06 \\cdot \\text{mean radius} + 0.5 \\cdot \\text{mean texture} + ... + \\: 0.04 \\cdot \\text{worst fractal dimension}$$\n",
    "\n",
    "    $$...$$\n",
    "\n",
    "\n",
    "    These new features are chosen to capture as much variability (information) in the original data as possible, while being **orthogonal** (uncorrelated, independent) to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6ee9c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It leverages the **singular value decomposition** from linear algebra:\n",
    "\n",
    "$$X = U \\Sigma V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b19bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bda7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` has an implementation of PCA.<br><small>Remember, PCA is an **unsupervised** technique, so the `'target'` column is irrelevant.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c96fca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once `fit`, `pca` can transform `X_train` into a **$p$-column matrix** in a way that retains the bulk of the information.<br><small>We'll use $p = 2$ to make it easy to visualize the resulting components.</small>\n",
    "\n",
    "    $$\\mathbb{R}^{426 \\times 30} \\rightarrow \\mathbb{R}^{426 \\times p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d573680",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accdb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = pca.transform(X_train)\n",
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb54dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above 2 new features are called \"principal components\".<br>Their values aren't directly interpretable (where do they come from?), but the new features are all uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7445439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(X_train_transformed.T) # Note that the correlation between the new features is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f31033",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91752201",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The resulting principal components aren't directly interpretable, but sometimes have intuitive interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pca = px.scatter(x=X_train_transformed[:, 0], y=X_train_transformed[:, 1], color=y_train.replace({0: 'benign', 1: 'malignant'}))\n",
    "fig_pca.update_layout(xaxis_title='PC 1', yaxis_title='PC 2', title='PCA Projection of Breast Cancer Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3dd92d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, the target variable **was not** used in computing the principal components.<br>However, clusters (in the target variable) _may_ naturally form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914243a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf2035",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We said that PCA aims to retain the **bulk** of the information in the original dataset. Let's be more precise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfab0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each principal component (new feature) has associated with it a \"score\", which describes the **proportion of variance in the original dataset that is captured by the PC**.<br><small>The score for PC $i$ is $\\frac{\\sigma_i^2}{n - 1}$, where $\\sigma_i$ is the $i$th entry in the diagonal matrix $\\Sigma$ from $X = U \\Sigma V^T$ and $n$ is the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f40366",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **scree plot** shows the explained variance ratio vs. the number of principal components, and is useful in helping choose the number of principal components to retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cd4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_large_p = PCA(n_components=10)\n",
    "pca_large_p.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da47c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(x=np.arange(1, 11), y=pca_large_p.explained_variance_ratio_)\n",
    "fig.update_layout(xaxis_title='Number of Principal Components', yaxis_title='Proportion of Variation Explained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9f336",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- PC 1 captures the most variability, then PC 2, then PC 3, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccb9fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c581e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To peek into how each PC was defined, we can look at the **loadings** vectors.<br><small>These correspond to the rows of the matrix $V^T$ in $X = U \\Sigma V^T$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c77359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 loadings vectors, since we created 2 PCs.\n",
    "# These are being rounded; they aren't sparse.\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa806474",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sometimes, the loadings vectors give intuitive definitions for the PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(y=pca.components_[0], x=pca.feature_names_in_, title='Coefficients used to construct PC 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7078e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(y=pca.components_[1], x=pca.feature_names_in_, title='Coefficients used to construct PC 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc031cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #2: Inconsistent Variance/Standard Deviation Formulas</h3>\n",
    "    \n",
    "Depending on the package, the default method for computing the variance of a dataset $x_1, x_2, ..., x_n$ may be the \"population\" formula:\n",
    "    \n",
    "$$\\sigma_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\:\\:\\:\\:\\:\\:\\:\\: (\\text{ddof}=0)$$\n",
    "    \n",
    "**or** the \"sample\" formula:\n",
    "    \n",
    "$$\\sigma_x^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\:\\:\\:\\:\\:\\:\\:\\: (\\text{ddof}=1)$$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of each new PC.\n",
    "X_train_transformed.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should also be the variance of each new PC!\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.var(axis=0, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfae5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Read the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7cd860",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Activity**: Look at the documentation for `sklearn.decomposition.PCA` by running the cell below (or going [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). Identify **2 unexpected operations** that it performs, without explicitly saying so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a473d13c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #3: Centering with PCA</h3>\n",
    "    \n",
    "Before applying PCA with singular value decomposition, you must **center each column** by subtracting each column's mean from all values in that column.\n",
    "    \n",
    "<br>\n",
    "\n",
    "Most PCA implementations handle this centering automatically, but always check your library's documentation to be sure. If you're using SVD directly rather than a dedicated PCA function, you'll need to center the data manually first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d48df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other dimensionality reduction techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fcd09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb82b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multidimensional scaling (MDS) is a dimensionality reduction technique that aims to **preserve pairwise (Euclidean) distances between points**.<br><small>If two individuals are close in the original space, they will be close in the lower-dimensional projected space, and if they are far in the original space, they will be far in the lower-dimensional projected space.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2, random_state=23)\n",
    "X_train_mds = mds.fit_transform(X_train) # Only uses X_train, not y_train!\n",
    "fig_mds = px.scatter(x=X_train_mds[:, 0], y=X_train_mds[:, 1], color=y_train.replace({0: 'benign', 1: 'malignant'}))\n",
    "fig_mds.update_layout(title='MDS Projection of Breast Cancer Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793de6a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that unlike PCA, the algorithm is not deterministic, so a random seed should be set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6811ad06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The features produced by MDS are not interpretable, so MDS is mostly a visualization tool, unless you have a good reason to believe that the distances between points in the original space are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a67f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MDS on Dissimilarity Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225bb0e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- MDS can be used directly on a **dissimilarity matrix** (i.e., a matrix of pairwise \"distances\" between points, where distances are calculated in some meaningful way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Ann Arbor', 'Toronto', 'San Diego', 'London', 'Copenhagen', 'Singapore']\n",
    "\n",
    "# Distances in miles (as the crow flies, approximate)\n",
    "# Sources: great circle distance calculators\n",
    "distances_miles = [\n",
    "    [0,     153,   1950,  3760,  4120,  9360],\n",
    "    [153,     0,   2130,  3550,  3910,  9460],\n",
    "    [1950, 2130,      0,  5470,  5790,  8800],\n",
    "    [3760, 3550,   5470,     0,   600,  6740],\n",
    "    [4120, 3910,   5790,   600,     0,  6130],\n",
    "    [9360, 9460,   8800,  6740,  6130,     0]\n",
    "]\n",
    "\n",
    "distances = pd.DataFrame(distances_miles, index=cities, columns=cities)\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dff31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also non-deterministic!\n",
    "mds_cities = MDS(n_components=2, random_state=23)\n",
    "cities_transformed = mds_cities.fit_transform(distances_miles)\n",
    "px.scatter(\n",
    "    x=cities_transformed[:, 0],\n",
    "    y=cities_transformed[:, 1],\n",
    "    text=cities\n",
    ").update_traces(textposition='middle right').update_layout(xaxis_range=[-10 ** 4, 10 ** 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6161f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $t$-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec7c32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- t-SNE (t-distributed stochastic neighbor embedding) is a dimensionality reduction technique that aims to **preserve local structure**, mostly for the purpose of forming clusters for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab236cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "X_train_tsne = tsne.fit_transform(X_train)\n",
    "fig_tsne = px.scatter(x=X_train_tsne[:, 0], y=X_train_tsne[:, 1], color=y_train.replace({0: 'benign', 1: 'malignant'}))\n",
    "fig_tsne.update_layout(title='t-SNE Projection of Breast Cancer Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fa98a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, the resulting features are not interpretable in the way PCA features are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7122c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b7555",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A good discussion of the differences between the three techniques can be found [here](https://orangedatamining.com/blog/pca-vs-mds-vs-t-sne/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85986e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In short:\n",
    "    - Use PCA to produce interpretable, new features.\n",
    "    - Use MDS and t-SNE to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig_compare = make_subplots(rows=1, cols=3, subplot_titles=[\"PCA\", \"MDS\", \"t-SNE\"])\n",
    "\n",
    "# PCA plot\n",
    "for trace in fig_pca.data:\n",
    "    fig_compare.add_trace(trace, row=1, col=1)\n",
    "fig_compare.update_xaxes(title_text=fig_pca.layout.xaxis.title.text, row=1, col=1)\n",
    "fig_compare.update_yaxes(title_text=fig_pca.layout.yaxis.title.text, row=1, col=1)\n",
    "\n",
    "# MDS plot\n",
    "for trace in fig_mds.data:\n",
    "    fig_compare.add_trace(trace, row=1, col=2)\n",
    "fig_compare.update_xaxes(title_text=fig_mds.layout.xaxis.title.text, row=1, col=2)\n",
    "fig_compare.update_yaxes(title_text=fig_mds.layout.yaxis.title.text, row=1, col=2)\n",
    "\n",
    "# t-SNE plot\n",
    "for trace in fig_tsne.data:\n",
    "    fig_compare.add_trace(trace, row=1, col=3)\n",
    "fig_compare.update_xaxes(title_text=fig_tsne.layout.xaxis.title.text, row=1, col=3)\n",
    "fig_compare.update_yaxes(title_text=fig_tsne.layout.yaxis.title.text, row=1, col=3)\n",
    "\n",
    "fig_compare.update_layout(height=400, width=1200, showlegend=False)\n",
    "fig_compare.show()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
