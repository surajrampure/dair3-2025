{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbeb756",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "from lec_utils import *\n",
    "import nb4 as util\n",
    "plotly.io.renderers.default = 'notebook'\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "full = load_breast_cancer()\n",
    "df = pd.DataFrame(full['data'], columns=full['feature_names'])\n",
    "df['target'] = 1 - full['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], random_state=23, stratify=df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ef119",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### DAIR-3 Workshop, Day 2 ‚Ä¢ Building Robust ML Models\n",
    "\n",
    "# Part 4: Model Evaluation\n",
    "\n",
    "**Instructor**: Suraj Rampure (rampure@umich.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ade72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Classifier evaluation.\n",
    "- Choosing a threshold.\n",
    "- Summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83cc94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00097fc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outcomes in binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02db002",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When performing **binary** classification, there are four possible outcomes.<br><small>Note: A \"positive prediction\" is a prediction of 1, and a \"negative prediction\" is a prediction of 0.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29e271",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We typically organize the four quantities above into a **confusion matrix**.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | True Negative (TN) ‚úÖ | False Positive (FP) ‚ùå |\n",
    "| **Actually Positive** | False Negative (FN) ‚ùå | True Positive (TP) ‚úÖ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c33ff3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in the four acronyms ‚Äì TP, FN, TN, FP ‚Äì the **first letter** is whether the prediction is correct, and the **second letter** is what the prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700f7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Depending on the situation, false negatives may be worse than false positives (or vice versa!).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde16cd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Accuracy of COVID tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a7b10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The results of 100 Michigan Medicine COVID tests are given below.\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| **Actually Positive** | FN = 8 ‚ùå | TP = 1 ‚úÖ |\n",
    "<center><i><small>Michigan Medicine test results</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a5c73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** What is the accuracy of the test?\n",
    "\n",
    "$$\n",
    "\\text{accuracy} = \\frac{\\text{# points classified correctly}}{\\text{# points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf075a5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** $$\\text{accuracy} = \\frac{TP + TN}{TP + FP + FN + TN} = \\frac{1 + 90}{100} = 0.91$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294b4b7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Followup:** At first, the test seems good. But, suppose we build a classifier that predicts that **nobody has COVID**. What would its accuracy be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba7485",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer to followup:** Also 0.91! There is severe **class imbalance** in the dataset, meaning that most of the data points are in the same class (no COVID). **Accuracy doesn't tell the full story!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c95960",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #7: Accuracy Can Mislead</h3>\n",
    "    \n",
    "In cases of class imbalance, accuracy can be misleading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e961a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 90 ‚úÖ | FP = 1 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 8</span> ‚ùå | <span style='color:orange'>TP = 1</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>Michigan Medicine test results</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef9b2bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** What proportion of individuals who actually have COVID did the test **identify**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983d5cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** $\\frac{1}{1 + 8} = \\frac{1}{9} \\approx 0.11$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27e46b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More generally, the **recall** of a binary classifier is the proportion of <span style='color:orange'><b>actually positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c8865",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compute recall, look at the <span style='color:orange'><b>bottom (positive) row</b></span> of the above confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03360b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall isn't everything, either!\n",
    "\n",
    "$$\\text{recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4754c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ü§î **Question:** Can you design a \"COVID test\" with perfect recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59f74a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **üôã Answer:** Yes ‚Äì **just predict that everyone has COVID!**\n",
    "\n",
    "| | Predicted Negative | Predicted Positive |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | FP = 91 ‚ùå |\n",
    "| <span style='color:orange'><b>Actually Positive</b></span> | <span style='color:orange'>FN = 0</span> ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef10a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{recall} = \\frac{TP}{TP + FN} = \\frac{9}{9 + 0} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86091881",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like accuracy, recall on its own is not a perfect metric. Even though the classifier we just created has perfect recall, it has 91 false positives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243ea36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "| | Predicted Negative | <span style='color:orange'>Predicted Positive</span> |\n",
    "| --- | --- | --- |\n",
    "| **Actually Negative** | TN = 0 ‚úÖ | <span style='color:orange'>FP = 91</span> ‚ùå |\n",
    "| **Actually Positive** | FN = 0 ‚ùå | <span style='color:orange'>TP = 9</span> ‚úÖ |\n",
    "\n",
    "<center><i><small>everyone-has-COVID classifier</small></i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54440e75",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **precision** of a binary classifier is the proportion of <span style='color:orange'><b>predicted positive instances</b></span> that are correctly classified. We'd like this number to be as close to 1 (100%) as possible.\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e570b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compute precision, look at the <span style='color:orange'><b>right (positive) column</b></span> of the above confusion matrix.<br><small>**Tip:** A good way to remember the difference between precision and recall is that in the denominator for üÖøÔ∏èrecision, both terms have üÖøÔ∏è in them (TP and FP).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45fdcc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the \"everyone-has-COVID\" classifier has perfect recall, but a precision of $\\frac{9}{9 + 91} = 0.09$, which is quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ab18f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® **Key idea:** There is a \"tradeoff\" between precision and recall. Ideally, you want both to be high. For a particular prediction task, one may be important than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6620db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision and recall\n",
    "\n",
    "<center><img src=\"images/Precisionrecall.svg.png\" width=30%></center>\n",
    "\n",
    "<center>(<a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\">source</a>)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff18633",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Discussion\n",
    "    \n",
    "$$\\text{precision} = \\frac{TP}{TP + FP} \\: \\: \\: \\:  \\: \\: \\: \\: \\text{recall} = \\frac{TP}{TP + FN}$$\n",
    "    \n",
    "- When might high **precision** be more important than high recall?\n",
    "- When might high **recall** be more important than high precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d61ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combining precision and recall\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188a78a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we care equally about a model's precision $PR$ and recall $RE$, we can combine the two using a single metric called the **F1-score**:\n",
    "\n",
    "$$\\text{F1-score} = \\text{harmonic mean}(PR, RE) = 2\\frac{PR \\cdot RE}{PR + RE}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ca27c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both F1-score and accuracy are overall measures of a binary classifier's performance. But remember, accuracy is misleading in the presence of class imbalance, and doesn't take into account the kinds of errors the classifier makes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a8bc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other evaluation metrics for binary classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e847fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We just scratched the surface! This [excellent table from Wikipedia](https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram) summarizes the many other metrics that exist.\n",
    "\n",
    "<center><img src='images/wiki-table.png' width=75%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482fdfc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you're interested in exploring further, a good next metric to look at is **true negative rate (i.e. specificity)**, which is the analogue of recall for true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5271fd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing a threshold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a3161",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Logistic regression for tumor classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a429e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's train a logistic regression model that uses mean radius **only** to predict tumor malignancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train[['mean radius']], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21c5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2c21b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The fit model makes predictions using $P(y_i = 1 | x_i) = \\sigma(-16.09 + 1.09 x_i)$,\n",
    "\n",
    "    where $x_i$ represents patient $i$'s mean radius."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00078d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Visually, the model's <span style=\"color:#097054\"><b>predicted probabilities</b></span> look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af20790",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df8f90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14781d82",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose $\\vec x_i$ is a **feature vector** containing the information about individual $i$, used for predicting the probability of a tumor, $P(y_i = 1 | \\vec x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa8052",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In order to classify $\\vec{x}_i$ as either yes ($y_i = 1$) or no ($y_i = 0$), we apply a **threshold** $T$ to the predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a185cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/threshold.svg\" width=600><small>With a threshold of $T = 0.6$, a predicted probability of 0.68 is classified as <span style=\"color:blue\">malignant (class 1)</span>,<br>and a predicted probability of 0.55 is classified as <span style=\"color:orange\">benign (class 0)</span>.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593db431",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More generally, if we pick a threshold of $T$, then any feature vector $\\vec{x}_i$ such that:\n",
    "\n",
    "    $$\\sigma(\\vec{w}^* \\cdot \\text{Aug}(\\vec{x}_i)) \\geq T$$ \n",
    "\n",
    "    is classified as class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c8035",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we choose the \"right\" threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df34090",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn`'s default threshold of $T = 0.5$ is **not** guaranteed to yield the highest **accuracy**!<br><small>To find optimal parameters, `sklearn` we minimized mean cross-entropy loss, and mean cross-entropy loss doesn't involve our threshold.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66b4ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing a custom threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b928d4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we want to use a custom threshold, we'll need to implement the logic ourselves.\n",
    "\n",
    "<center><img src=\"images/threshold.svg\" width=300></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_thresholded(X, T):\n",
    "    '''Calls model_logistic_multiple.predict_proba.\n",
    "       For each P(y_i = 1 | x_i), returns 1 if >= T and 0 if < T.'''\n",
    "    probs = model.predict_proba(X)[:, 1]\n",
    "    return (probs >= T).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c10e87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we can choose any threshold we'd like, and compute the accuracy of the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_thresholded([[14.5]], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf574aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_thresholded([[14.5]], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a146c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_thresholded(X_train[['mean radius']], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989837a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy for the threshold T = 0.4.\n",
    "(predict_thresholded(X_train[['mean radius']], 0.4) == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316991a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60420c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy is defined as:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{# points classified correctly}}{\\text{# points}} = \\frac{TP + TN}{TP + FP + FN + TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620688eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's **training** accuracy change as the threshold changes?<br><small>Note that we'd see a similar trend with test accuracy, too.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train[['mean radius']], y_train, 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921035f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The threshold with the best training accuracy (among the thresholds we tried) is $T = 0.59$, which has a training accuracy of 88.9\\%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4abc7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember that 63\\% of tumors in the training set are benign, so we can achieve a 63\\% training accuracy just by always predicting \"benign\"! This means that a good model's accuracy should be much higher than 63\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67f87a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metrics for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7369a46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<center><small>Here, a false positive ($FP$) is when we predict a tumor is malignant, when it is benign.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77423feb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "<center><small>Here, a false negative ($FN$) is when we predict that a tumor is benign, when it is malignant?.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66609e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A binary classifier's **confusion matrix** displays its number of true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.show_confusion(X_train[['mean radius']], y_train, T=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53f80f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, we're predicting whether tumors are malignant (1) or benign (0). **Which is worse: a false positive or a false negative?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4ebe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe how the values in the confusion matrix change as the threshold changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(lambda T: util.show_confusion(X_train, y_train, T), T=(0, 1.01, 0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4cc745",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34813ef4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Precision is defined as:\n",
    "\n",
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<center><small>Here, a false positive ($FP$) is when we predict a tumor is malignant, when it is benign.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbc58b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's training **precision** change as the threshold changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train[['mean radius']], y_train, 'Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc04e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the \"bar\" is higher to predict 1, then we will have fewer positives in general, and thus fewer false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c054b96",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As the **threshold increases** ‚¨ÜÔ∏è, the denominator in $\\text{precision} = \\frac{TP}{TP + FP}$ will decrease, and so **precision tends to increase** ‚¨ÜÔ∏è.<br><small>There are some cases where a slightly higher threshold led to a slightly lower precision; why?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe599fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff0195",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall is defined as:\n",
    "\n",
    "    $$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "<center><small>Here, a false negative ($FN$) is when we predict that a tumor is benign, when it is malignant.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25fd88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's training **recall** change as the threshold changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train[['mean radius']], y_train, 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f054603",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the denominator in $\\text{recall} = \\frac{TP}{\\text{# actually positive}}$ is constant. As the **threshold increases** ‚¨ÜÔ∏è:\n",
    "    - true positives get converted to false negatives, so\n",
    "    - the numerator of recall ($TP$) decreases, and so\n",
    "    - **recall decreases** ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a3226",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision vs. recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b02705",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can visualize how precision and recall vary **together**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pr_curve(X_train[['mean radius']], y_train).update_layout(width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006b6d7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The curve above is called a **PR curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42a46c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Given the information above, what threshold would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcb5382",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer**: The threshold whose point is closest to the **top right corner** of the plot above. <br><small>Why? The top right corner is where precision = 1 and recall = 1, and we want both to be high.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449396a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec567abd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A more popular variant of the PR curve is the **ROC curve**.<br><small>ROC stands for \"receiver operating characteristic.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8404c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A ROC curve plots true positive rate (TPR) vs. false positive rate (FPR) for all possible thresholds, where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312f23c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{\\text{true positive rate (TPR)} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN} = \\text{recall}}_\\text{we want this to be close to 1!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d1ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{\\text{false positive rate (FPR)} = \\frac{FP}{\\text{# actually negative}} = \\frac{FP}{FP + TN}}_\\text{we want this to be close to 0!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80a86f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "util.draw_roc_curve(X_train[['mean radius']], y_train).update_layout(width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d0484",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we care about TPR and FPR equally, the best threshold is the one whose point is closest to the **top left corner** in the plot above.<br><small>Why? The top left corner is where $TPR = 1$ and $FPR = 0$, and we want $TPR$ to be high and $FPR$ to be low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a8c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run this cell to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Single Feature', 'All Features'))\n",
    "\n",
    "# Fit logistic regression models and calculate ROC curves\n",
    "# Single feature model\n",
    "lr_single = LogisticRegression()\n",
    "lr_single.fit(X_train[['mean radius']], y_train)\n",
    "y_pred_proba_single = lr_single.predict_proba(X_train[['mean radius']])[:, 1]\n",
    "fpr_single, tpr_single, _ = roc_curve(y_train, y_pred_proba_single)\n",
    "auc_single = auc(fpr_single, tpr_single)\n",
    "\n",
    "# All features model\n",
    "lr_all = LogisticRegression()\n",
    "lr_all.fit(X_train, y_train)\n",
    "y_pred_proba_all = lr_all.predict_proba(X_train)[:, 1]\n",
    "fpr_all, tpr_all, _ = roc_curve(y_train, y_pred_proba_all)\n",
    "auc_all = auc(fpr_all, tpr_all)\n",
    "\n",
    "# Add filled areas under the curves\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_single, \n",
    "        y=tpr_single,\n",
    "        fill='tonexty',\n",
    "        fillcolor='rgba(128, 0, 128, 0.3)',  # Purple with alpha\n",
    "        line=dict(color='purple', width=2),\n",
    "        name=f'Single Feature (AUC = {auc_single:.3f})',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_all, \n",
    "        y=tpr_all,\n",
    "        fill='tonexty',\n",
    "        fillcolor='rgba(0, 128, 0, 0.3)',  # Green with alpha\n",
    "        line=dict(color='green', width=2),\n",
    "        name=f'All Features (AUC = {auc_all:.3f})',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Add diagonal reference lines\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1], \n",
    "        y=[0, 1],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='gray'),\n",
    "        name='Random Classifier',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1], \n",
    "        y=[0, 1],\n",
    "        mode='lines',\n",
    "        line=dict(dash='dash', color='gray'),\n",
    "        name='Random Classifier',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Add AUC annotations\n",
    "fig.add_annotation(\n",
    "    x=0.6, y=0.3,\n",
    "    text=f\"AUC = {auc_single:.3f}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=14, color=\"purple\"),\n",
    "    bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "    bordercolor=\"purple\",\n",
    "    borderwidth=1,\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0.6, y=0.3,\n",
    "    text=f\"AUC = {auc_all:.3f}\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=14, color=\"green\"),\n",
    "    bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "    bordercolor=\"green\",\n",
    "    borderwidth=1,\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    showlegend=False, \n",
    "    width=1000, \n",
    "    height=500, \n",
    "    title='Comparison of ROC Curves'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", range=[0, 1])\n",
    "fig = fig.update_yaxes(title_text=\"True Positive Rate\", range=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f655fe8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cbd16d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we draw two ROC curves:\n",
    "    - On the **left**, for the model that uses just mean radius to predict malignancy.\n",
    "    - On the **right**, for a model that uses all 30 features to predict malignancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a477e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A common metric for the quality of a binary classifier is the **area under curve (AUC)** for the ROC curve.<br><small>Larger values are better!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cb12e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC curves vs. PR curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a158ee2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Discuss**: Suppose we're deciding between Model A and Model B, both of which are models that predict probabilities (like logistic regression), and suppose **both** of the following are true:\n",
    "\n",
    "    $$\\text{ROC-AUC(Model A)} > \\text{ROC-AUC(Model B)}$$\n",
    "    $$\\text{PR-AUC(Model A)} < \\text{PR-AUC(Model B)}$$\n",
    "\n",
    "    In what scenario would we choose Model A? Model B? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a702db86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- See [**here**](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves) for a good discussion on the differences between PR curves and ROC curves.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8977968",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d4300",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In cases of class imbalance, we've discussed:\n",
    "    - Setting `stratify` when performing a `train_test_split`.\n",
    "    - Choosing a metric (precision, recall) that better suits the task at hand.\n",
    "    - Using PR curves instead of ROC curves to choose a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e41ebc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another solution is to change how the model itself is trained, to account for the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bbb81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.show_balancing_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89210343",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By setting `class_weight='balanced'`, the minority class is given a higher \"cost\" in the model's optimization routine, effectively duplicating the minority class.\n",
    "\n",
    "    <br>\n",
    "\n",
    "    **If using `class_weight='balanced'`, refer to the documentation to see how the weights are assigned!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945f0e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083e989",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59501281",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Missing random seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395b187",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Inconsistent variance/standard deviation formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5f0f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Centering with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a0411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Lack of cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e692ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "5. Unreported regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12863c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "6. Leakage with feature standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50117bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "7. Accuracy can mislead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71424353",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Time-permitting: TRIPOD+AI Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c703a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Read the **TRIPOD+AI checklist [here](https://www.tripod-statement.org/wp-content/uploads/2019/12/TRIPODAI_checklist.pdf)**, in particular, sections 5, 7, 9, 12-16. How has our work today informed how we'd approach these guidelines?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
