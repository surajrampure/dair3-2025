{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbeb756",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "from lec_utils import *\n",
    "plotly.io.renderers.default = 'notebook'\n",
    "\n",
    "def show_cv_slides():\n",
    "    src = \"https://docs.google.com/presentation/d/e/2PACX-1vTydTrLDr-y4nxQu1OMsaoqO5EnPEISz2VYmM6pd83ke8YnnTBJlp40NfNLI1HMgoaKx6GBKXYE4UcA/embed?start=false&loop=false&delayms=60000&rm=minimal\"\n",
    "    display(IFrame(src, width=900, height=361))\n",
    "    \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "full = load_breast_cancer()\n",
    "df = pd.DataFrame(full['data'], columns=full['feature_names'])\n",
    "df['target'] = 1 - full['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], random_state=23, stratify=df['target'])\n",
    "\n",
    "def format_results(searcher):\n",
    "    cv = sum([1 if ('split' in s and '_test_score' in s) else 0 for s in list(searcher.cv_results_.keys())])\n",
    "    deg = searcher.cv_results_['mean_fit_time'].shape[0]\n",
    "    \n",
    "    df = pd.DataFrame(np.vstack([searcher.cv_results_[f'split{i}_test_score'] for i in range(cv)]))\n",
    "    df.index = [f'Fold {i}' for i in range(1, cv + 1)]\n",
    "    df.columns = [f'p={j}' for j in range(1, deg + 1)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ef119",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### DAIR-3 Workshop, Day 2 ‚Ä¢ Building Robust ML Models\n",
    "\n",
    "# Part 3: Model Selection\n",
    "\n",
    "**Instructor**: Suraj Rampure (rampure@umich.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ade72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Cross-validation.\n",
    "- Other considerations.\n",
    "    - Regularization.\n",
    "    - Feature standardization.\n",
    "    - Data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156eb8d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f0297",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing between models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e4b46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Often, we want to choose between several different candidate models.\n",
    "    - Logistic regression using all 30 features, or just 29, 28, 27, ...\n",
    "    - Logistic regression using PCA with $p = 2$, or $p = 3$, or $p = 4$...\n",
    "    - Decision tree with a depth of 3, or 4, or 5, or 10...\n",
    "\n",
    "    <br>\n",
    "\n",
    "    The value of $p$ in PCA, or the depth of a decision tree, is a **hyperparameter** of the model. Think of hyperparameters as **knobs** üéõÔ∏è we get to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e4057",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **One idea**: Train each candidate model on the training set, compute test set accuracy, and pick the model with the best test set accuracy.\n",
    "\n",
    "<center><img src=\"images/train-test-first.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1099a30",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Why is this problematic?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427cc42",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Because, we are **overfitting to the test set** ‚Äì the best hyperparameter for the test set might not be the best hyperparameter for a totally unseen dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c7b052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea: A single validation set\n",
    "\n",
    "<center><img src='images/train-test-val.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344ac51",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Split the data into three sets: <span style='color: blue'><b>training</b></span>, <span style='color: green'><b>validation</b></span>, and <span style='color: orange'><b>test</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d7d91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. For each candidate model, <span style='color: blue'><b>train</b></span> the model only on the <span style='color: blue'><b>training set</b></span>, and <span style='color: green'><b>evaluate</b></span> the model's performance on the <span style='color: green'><b>validation set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66367d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Find the candidate with the best <span style='color: green'><b>validation</b></span> performance (e.g. highest accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad93210",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Retrain the final model on the <span style='color: blue'><b>training</b></span> and <span style='color: green'><b>validation</b></span> sets, and report its performance on the <span style='color: orange'><b>test set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e49e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Issue**: This strategy is too dependent on the <span style='color: green'><b>validation</b></span> set, which may be small and/or not a representative sample of the data. **We're not going to do this.** ‚ùå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9131a04",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A better idea: $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdcd00c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of relying on a single <span style='color: green'><b>validation</b></span> set, we can create $k$ <span style='color: green'><b>validation</b></span> sets, where $k$ is some positive integer (5 in the example below).\n",
    "\n",
    "<center><img src='images/k-fold.png' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd266d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since each data point is used for <span style='color: blue'><b>training</b></span> $k-1$ times and <span style='color: green'><b>validation</b></span> once, the (averaged) <span style='color: green'><b>validation</b></span> performance should be a good metric of a model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99552b54",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $k$-fold cross-validation (or simply \"cross-validation\") is **the** gold standard for choosing between candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b780cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #4: Lack of Cross-Validation</h3>\n",
    "    \n",
    "Be weary of studies that choose models based on their performance on a single test set, as their test set performance will be overly optimistic.\n",
    "\n",
    "While test set performance may be all that is reported, verify that cross-validation was used to actually select the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573ad02b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrating $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f65b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To illustrate $k$-fold cross-validation, let's use the following example dataset with $n = 12$ rows.<br><small>Suppose this dataset represents our **training set**, i.e. suppose we already performed a train-test split.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame().assign(x=range(0, 120, 10),\n",
    "                                      y=[9, 1, 58, 3, 6, 4, -2, 8, 1, 10, 1.1, -45])        \n",
    "\n",
    "display_df(training_data, rows=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3914b31",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we choose $k = 4$. Then, each fold has $\\frac{12}{4} = 3$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21473edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cv_slides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef864c36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modeling pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434eaa8e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's return to the breast cancer dataset.<br>In the previous notebook, we used a scree plot to choose the number of principal components to use. This did not factor in model performance (e.g. accuracy) at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd4cf3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's build a **Pipeline object** that first performs PCA with $p$ components, then fits a logistic regression model. To choose $p$, we'll use cross-validation, choosing the number of components that maximizes average cross-validated accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e30f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(PCA(), LogisticRegression()) # Note that we haven't specified n_components!\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae27d09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133ace2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `GridSearchCV` takes in:\n",
    "    - an **un-`fit`** Pipeline object, and\n",
    "    - a **dictionary** of hyperparameter values to try,\n",
    "    \n",
    "  and performs $k$-fold cross-validation to find the **combination of hyperparameters** with the best average validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "searcher = GridSearchCV(\n",
    "    estimator=make_pipeline(PCA(), LogisticRegression()),\n",
    "    param_grid={\n",
    "        'pca__n_components': range(1, 31),\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5100ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The orange diagram indicates the estimator (which GridSearchCV is) has yet to be fit.\n",
    "searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501fb73",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that keeping $p = 5$ principal components leads to the best validation accuracy, so that's what we should do (among the 30 choices; perhaps there are better models to choose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92747817",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting the results of $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b37ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's peek under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399b5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = format_results(searcher) # Helper function defined at the top of the notebook.\n",
    "acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce0a39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that for each choice of $p$ (our hyperparameter), we have **five** accuracies, one for each \"fold\" of the data. This means that in total, $5 \\cdot 30 = 150$ models were trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b364f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, our goal is to choose the $p$ with the **highest average** validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = acc_df.mean(axis=0).iloc[:31].plot(kind='line', title='Average Validation Accuracy')\n",
    "fig.update_layout(xaxis_title='Degree', yaxis_title='Average Validation Accuracy', showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen automatically by sklearn.\n",
    "acc_df.mean(axis=0).idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a535c3dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that if we didn't perform $k$-fold cross-validation, but instead just used a single validation set, we may have ended up with a different result (so be weary of single-validation-set studies!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c870c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1e55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now that a model has been chosen..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35584c57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p = 5$ is the optimal number of principal components, according to our cross-validation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3754f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, it is **not necessarily** the value of $p$ that has the highest test set accuracy! We are **intentionally not looking** at the test set accuracy of each possible $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa346684",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Observe:\n",
    "    - The **test set accuracy** of the $p = 5$ model is **higher** than the all-30-features-but-no-PCA model, but\n",
    "    - the **training set accuracy** of the $p = 5$ model is **lower**.\n",
    "    <br>\n",
    "    \n",
    "    **Why?** Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32068eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all_features = LogisticRegression()\n",
    "model_all_features.fit(X_train, y_train)\n",
    "model_all_features.score(X_test, y_test)\n",
    "\n",
    "print(f'p = 5 PCA, then Logistic Regression\\nTraining Accuracy: {round(searcher.score(X_train, y_train), 4)} \\t Testing Accuracy: {round(searcher.score(X_test, y_test), 4)}\\n\\n')\n",
    "print(f'All 30 Features Logistic Regression\\nTraining Accuracy: {round(model_all_features.score(X_train, y_train), 4)} \\t Testing Accuracy: {round(model_all_features.score(X_test, y_test), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0edd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training and test set accuracy vs. model complexity\n",
    "\n",
    "(See annotated slides.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e553a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î</h3>\n",
    "        \n",
    "- Suppose you have a training dataset with 1000 rows.\n",
    "- You want to decide between 20 hyperparameters for a particular model.\n",
    "- To do so, you perform 10-fold cross-validation.\n",
    "- **How many times is the first row in the training dataset (`X.iloc[0]`) used for training a model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9127fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing between many models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73501c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Often, we want to use cross-validation to choose between different model types, not just different hyperparameter values.<br>Doing so is slightly more complicated in code, but the general idea is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30c8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pipes = {\n",
    "    'Logistic Regression (All 30 Features)': LogisticRegression(),\n",
    "    'Logistic Regression (Mean Radius Only)': make_pipeline(\n",
    "        FunctionTransformer(lambda X: X[['mean radius']], validate=False), \n",
    "        LogisticRegression()\n",
    "    ),\n",
    "    'Logistic Regression (5 PCA Components)': make_pipeline(PCA(n_components=5), LogisticRegression()),\n",
    "    'Decision Tree (Depth 3)': DecisionTreeClassifier(max_depth=3, random_state=23),\n",
    "    'Decision Tree (Depth 10)': DecisionTreeClassifier(max_depth=10, random_state=23),\n",
    "    'Random Forest (100 Estimators)': RandomForestClassifier(n_estimators=100, random_state=23),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499fa9d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we will have to call `GridSearchCV` multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a991292",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Average Training Accuracy', 'Average Validation Accuracy'])\n",
    "\n",
    "for pipe in pipes:\n",
    "    fitted = GridSearchCV(\n",
    "        pipes[pipe],\n",
    "        param_grid={}, # No hyperparameters, but we could have them.\n",
    "        scoring='accuracy',\n",
    "        cv=5, # Change this and see what happens!\n",
    "        return_train_score=True # So that we can compute training accuracies, too.\n",
    "    )\n",
    "    \n",
    "    fitted.fit(X_train, y_train)\n",
    "    results.loc[pipe] = [fitted.cv_results_['mean_train_score'][0], fitted.cv_results_['mean_test_score'][0]]\n",
    "    \n",
    "cancer_models_summarized = (\n",
    "    results\n",
    "    .sort_values('Average Training Accuracy')\n",
    "    .plot(kind='barh', barmode='group', width=1000)\n",
    "    .update_layout(xaxis_title='Accuracy', yaxis_title='Model')\n",
    ")\n",
    "\n",
    "cancer_models_summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c09cf77",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model is most likely to perform best in practice? Which model has the highest bias? Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b3ad4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2427b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We care about how well our models **generalize** to unseen data.<br><small><ul><li>The more complex a model is, the more it will **overfit** to the noise in the training data, and have high **model variance**.</li><li>The less complex a model is, the more it will **underfit** the training data, and have high **bias**.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fac08a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To choose the model that is **most likely to generalize well**:\n",
    "    1. Split the data into two sets: <span style='color: blue'><b>training</b></span> and <span style='color: orange'><b>test</b></span>.\n",
    "    2. Use only the <span style='color: blue'><b>training</b></span> data when designing, training, and tuning the model.\n",
    "        - Use <span style='color: green'><b>$k$-fold cross-validation</b></span> to choose between candidate models and to estimate a model's ability to generalize.\n",
    "        - Do not ‚ùå look at the <span style='color: orange'><b>test</b></span> data in this step!\n",
    "    3. Commit to your final model and train it using the entire <span style='color: blue'><b>training</b></span> set.\n",
    "    4. Test the data using the <span style='color: orange'><b>test</b></span> data. If the performance (e.g. accuracy) is not acceptable, return to step 2.\n",
    "    5. Finally, train on **all available data** and ship the model to production! üõ≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662df112",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® This is the process you should **always** use! üö® "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de398461",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?\n",
    "\n",
    "Let's look at a few common sources of error in the model selection process:\n",
    "\n",
    "- Regularization and logistic regression.\n",
    "- Feature standardization.\n",
    "- Data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a44cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other considerations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9fc0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Read the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a5fc3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Activity**: Look at the documentation for `sklearn.linear_model.LogisticRegression` by running the cell below (or going [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). Identify **1 unexpected operation** that it performs, without explicitly saying so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd53641",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6aa406",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For context, read the documentation for the `LinearRegression` class as well (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b66f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #5: Unreported Regularization</h3>\n",
    "    \n",
    "Logistic regression in `sklearn` applies $L_2$ regularization by default, using a regularization penalty of $\\lambda = 1$. \n",
    "    \n",
    "<br>\n",
    "    \n",
    "There are some (arguably good) reasons for this, but keep this in mind when using it (and when reviewing others' work that uses logistic regression!).\n",
    "    \n",
    "<br>\n",
    "    \n",
    "If you do want to use regularization, use `LogisticRegressionCV` or `GridSearchCV` to choose a regularization hyperparameter ‚Äì **don't** just use the default. To turn off regularization, set `C=np.inf` as an argument in `LogisticRegression`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559af8f",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">By default, logistic regression in scikit-learn runs w L2 regularization on and defaulting to magic number C=1.0. How many millions of ML/stats/data-mining papers have been written by authors who didn&#39;t report (&amp; honestly didn&#39;t think they were) using regularization?</p>&mdash; Zachary Lipton (@zacharylipton) <a href=\"https://twitter.com/zacharylipton/status/1167298276686589953?ref_src=twsrc%5Etfw\">August 30, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36a510",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45dfcf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Some** models benefit from having standardized features, in which, separately for each feature (column), we apply the transformation:\n",
    "\n",
    "    $$x_i \\rightarrow \\frac{x_i - \\bar{x}}{\\sigma_x}$$\n",
    "\n",
    "    where $\\bar{x}$ and $\\sigma_x$ are the mean and standard deviation of the column, respectfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "make_pipeline(StandardScaler(), PCA(n_components=5), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed4fca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Discuss**: When might we standardize, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882c2ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "pipes = {\n",
    "    'Logistic Regression (All 30 Features)': LogisticRegression(),\n",
    "    'Logistic Regression (All 30 Features, Standardized)': make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'Logistic Regression (All 30 Features, No Regularization)': LogisticRegression(C=np.inf),\n",
    "    'Logistic Regression (All 30 Features, Standardized, No Regularization)': make_pipeline(StandardScaler(), LogisticRegression(C=np.inf)),\n",
    "    'Logistic Regression (5 PCA Components)': make_pipeline(PCA(n_components=5), LogisticRegression()),\n",
    "    'Logistic Regression (5 PCA Components, Standardized)': make_pipeline(StandardScaler(), PCA(n_components=5), LogisticRegression()),\n",
    "    'Decision Tree (Depth 3)': DecisionTreeClassifier(max_depth=3, random_state=23),\n",
    "    'Decision Tree (Depth 3, Standardized)': make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=3, random_state=23)),\n",
    "    'Decision Tree (Depth 10)': DecisionTreeClassifier(max_depth=10, random_state=23),\n",
    "    'Decision Tree (Depth 10, Standardized)': make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=10, random_state=23)),\n",
    "    'Random Forest (100 Estimators)': RandomForestClassifier(n_estimators=100, random_state=23),\n",
    "    'Random Forest (100 Estimators, Standardized)': make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=23)),\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(columns=['Training Accuracy'])\n",
    "\n",
    "for pipe in pipes:\n",
    "    model = pipes[pipe]\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    # test_acc = model.score(X_test, y_test)\n",
    "    results.loc[pipe] = [train_acc]\n",
    "\n",
    "# Define color pairs for each model type (raw/std, no_reg/std_no_reg, etc.)\n",
    "# There are 6 pairs, so 6 colors, each repeated twice for the pair\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_pairs = [\n",
    "    \"#1f77b4\", \"#1f77b4\",  # raw_features, std_raw_features\n",
    "    \"#ff7f0e\", \"#ff7f0e\",  # raw_features_no_reg, std_raw_features_no_reg\n",
    "    \"#2ca02c\", \"#2ca02c\",  # pca_5, std_pca_5\n",
    "    \"#d62728\", \"#d62728\",  # dt_depth_3, std_dt_depth_3\n",
    "    \"#9467bd\", \"#9467bd\",  # dt_depth_10, std_dt_depth_10\n",
    "    \"#8c564b\", \"#8c564b\",  # rf_n_estimators_100, std_rf_n_estimators_100\n",
    "]\n",
    "\n",
    "# The results are indexed by the keys in pipes, so we need to reverse the color list if we reverse the DataFrame\n",
    "reversed_colors = color_pairs[::-1]\n",
    "\n",
    "std_model_comparison = (\n",
    "    results\n",
    "    .iloc[::-1]\n",
    "    .plot(kind='barh', color=reversed_colors, width=1000)\n",
    ")\n",
    "\n",
    "std_model_comparison = std_model_comparison.update_layout(showlegend=False, xaxis_title='Training Accuracy', yaxis_title='Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef7e30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison of model performance, with and without standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b83762",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c51159",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Data leakage** occurs when information about the test set is used to train a model, even in the presence of a train-test split and cross-validation. This will lead to overly optimistic estimates of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fa8ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Discuss**: How might data leakage occur? Give examples in the context of patient data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7bee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74fd9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\"><h3>Warning #6: Leakage with Feature Standardization</h3>\n",
    "    \n",
    "If performing feature standardization, do so AFTER performing a train-test split, not before! If you use `StandardScaler` as part of a Pipeline object that you `fit` directly, this is handled for you.\n",
    "    \n",
    "If you standardize features before splitting the data, then the means and variances of the test set are encoded in the training set, and the test set won't be a truly fresh sample of unseen data, and test set performance will be overly optimistic.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604c320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leakage and feature standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c6b5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Good**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97972f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, features are standardized using only the information in X_train.\n",
    "# When model.predict is called, features are transformed using the means and variances\n",
    "# of the columns in X_train.\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2162e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Bad**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b991061",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_y = df.iloc[:, :-1]\n",
    "features_std = StandardScaler().fit_transform(without_y)\n",
    "# Often, this is implemented manually, e.g.\n",
    "# features_std = (without_y - without_y.mean(axis=0)) / without_y.std(axis=0)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_std, df.iloc[:, -1], random_state=23, stratify=df.iloc[:, -1])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
